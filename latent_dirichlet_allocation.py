# -*- coding: utf-8 -*-
"""latent_dirichlet_allocation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u-yhU06h_z6SY9dqpyfHw0leuJqHvHhE
"""

# to understand the theory behind the working of LDA
# go have a look at this youtube link

https://www.youtube.com/watch?v=BaM1uiCpj_E

# inputs to LDA is a document term matrix(DTM) which is nothing but a word count or bag of word
# number of topic into which we want to cluster these articles

import pandas as pd
from sklearn import decomposition
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

import nltk
nltk.download('punkt')

# getting the data 
data = pd.read_csv("https://www.consumerfinance.gov/data-research/consumer-complaints/search/api/v1/?date_received_max=2021-04-21&date_received_min=2021-01-21&field=all&format=csv&no_aggs=true&size=110667",sep=",")
data.head()

data.info()

new_data = data[~data['Consumer complaint narrative'].isnull()]

new_data.shape

cfpp = new_data[['Consumer complaint narrative','Product','Company']].rename(columns={'Consumer complaint narrative':'complaints'})
cfpp.head(4)



# now after filtering the data we tokenize it for which ive special function which ill use along with tfidfvectorizer

def tokenize(text):
  tokens = [token for token in nltk.word_tokenize(text) if (len(token) > 3 and len(token.strip('Xx/')) > 2)]
  return tokens

vectorizer = TfidfVectorizer(tokenizer=tokenize,stop_words='english',max_df=0.75,min_df=50,max_features=10000,use_idf=False,norm=None)
tf_vectors = vectorizer.fit_transform(cfpp['complaints'])

tf_vectors.A

vectorizer.get_feature_names()

# we got the faetures names and we have the documents(complaints)
# lets use LDA now

lda = decomposition.LatentDirichletAllocation(n_components=6,max_iter = 15,learning_method='online',learning_offset=50,random_state=222)
w1 = lda.fit_transform(tf_vectors)

h1 = lda.components_ #number of times a word was assigned to a topic / normalized array, [n_components, n_features] number of timess n_feaetures was assigned to n_coponents
h1.shape

vocab = np.array(vectorizer.get_feature_names())
vocab.shape

# from h1 shape and num of features from vectorizer we see that
# h1 has 6 topics ahnd each topic has probabilty orhaving each word from vocab
# and hence we can get top X topics from each topic easily

# from above shape we can see that a total of 2664 words were assigned to 6 topics

# getting the top X words from each topic 
import numpy as np

num_top_words = 15
vocab = np.array(vectorizer.get_feature_names())
get_top_words = lambda t : [vocab[index] for index in np.argsort(t)[:num_top_words]]
topic_words = [get_top_words(t) for t in h1]

top_words = [",  ".join(word) for word in topic_words]

top_words

# lets see what W1 = lda.fit_trasnform() looks like
w1.shape

cfpp.shape

# well its bsically a probabilty distribution between document and topics

# create a dataframe for this

cols = ['topic_'+str(i) for i in range(lda.n_components)]
docs = ['doc_'+str(i) for i in range(cfpp.shape[0])]
dataframe = pd.DataFrame(data = np.round(w1,2),index = docs,columns=cols)
dataframe.head()

dataframe['dominant_topic'] = np.argmax(dataframe.values,axis=1)

dataframe.head()